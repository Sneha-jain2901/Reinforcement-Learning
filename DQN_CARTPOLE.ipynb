{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "JvBLYY3HNlrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bf4f85-fbe4-49ae-c9fa-677b45db2552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/958.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEYF00mpMaqK"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "from IPython.display import clear_output,display\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import Model, Input\n",
        "from keras.models import clone_model\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=gym.make(\"CartPole-v1\",render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "6pC3SOH7N62r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#policy\n",
        "#here state is going to be a vector of 4 elements\n",
        "def policy(state,epsilon=0.0):\n",
        "  #here we'll batch it up i.e. convert it into a batch of oonly one state\n",
        "  # tenor of 1 to tensor of [1,4]\n",
        "  action = tf.argmax(q_net(tf.expand_dims(state,axis=0))[0],output_type=tf.int32)\n",
        "  if(tf.random.uniform(shape=(),maxval=1)<=epsilon):\n",
        "    action = tf.random.uniform(shape=(),maxval=2,dtype=tf.int32)\n",
        "  return action"
      ],
      "metadata": {
        "id": "t64gmbsiRqiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reward(state):\n",
        "    # -1 Reward for every step\n",
        "    reward = -1.0\n",
        "\n",
        "    # Reward +1.0 when,\n",
        "    # Cart Position is between +-0.5,\n",
        "    # Cart Velocity is within +-1,\n",
        "    # Pole Angle is lesser than 4 degrees, and\n",
        "    # pole angular velocity is lower than 15%\n",
        "    if -0.5 <= state[0] <= 0.5 and -1 <= state[1] <= 1 and -0.07 <= state[2] <= 0.07 and -0.525 <= state[3] <= 0.525:\n",
        "        reward = 1.0\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "Bej3nREacCYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_transition(transition):\n",
        "  if(len(replay_buffer)>max_transition):\n",
        "    replay_buffer.pop(0)\n",
        "  replay_buffer.append(transition)"
      ],
      "metadata": {
        "id": "OzJMVf9JTrG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_transition(batch_size=16):\n",
        "  random_indices=tf.random.uniform(shape=(batch_size,),minval=0,maxval=len(replay_buffer),dtype=tf.int32)\n",
        "  sampled_current_states=[]\n",
        "  sampled_actions=[]\n",
        "  sampled_rewards=[]\n",
        "  sampled_next_states=[]\n",
        "  sampled_terminals=[]\n",
        "\n",
        "  for index in random_indices:\n",
        "    sampled_current_states.append(replay_buffer[index][0])\n",
        "    sampled_actions.append(replay_buffer[index][1])\n",
        "    sampled_rewards.append(replay_buffer[index][2])\n",
        "    sampled_next_states.append(replay_buffer[index][3])\n",
        "    sampled_terminals.append(replay_buffer[index][4])\n",
        "  return tf.convert_to_tensor(sampled_current_states), tf.convert_to_tensor(sampled_actions), tf.convert_to_tensor(sampled_rewards), tf.convert_to_tensor(sampled_next_states), tf.convert_to_tensor(sampled_terminals)\n"
      ],
      "metadata": {
        "id": "J4Wh4iKgUCkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_input =Input(shape=(4,))\n",
        "x=Dense(62,activation=\"relu\")(net_input)\n",
        "x=Dense(62,activation=\"relu\")(x)\n",
        "output=Dense(2,activation=\"linear\")(x)\n",
        "q_net=Model(inputs=net_input,outputs=output)\n",
        "\n",
        "#learning rate iis managed by optimizer instead of alpha\n",
        "q_net.compile(optimizer=\"adam\")\n",
        "loss_fn=tf.keras.losses.Huber()\n",
        "target_net = clone_model(q_net)\n",
        "\n",
        "def get_q_values(states):\n",
        "    return tf.reduce_max(q_net(states), axis=1)\n",
        "\n",
        "\n",
        "epsilon=1.0\n",
        "epsilon_decay=1.005\n",
        "gamma=0.99\n",
        "episodes=400\n",
        "batch_size=64\n",
        "max_transition=10000\n",
        "replay_buffer=[]\n",
        "target_update_after=4\n",
        "LEARN_AFTER_STEPS=3\n",
        "# Gathering Random initial states for Average Q Metric\n",
        "random_states = []\n",
        "done = False\n",
        "i = 0\n",
        "state,_ = env.reset()\n",
        "while i < 20 and not done:\n",
        "    random_states.append(state)\n",
        "    state, _, done, _ = env.step(policy(state).numpy())\n",
        "    i += 1\n",
        "\n",
        "random_states = tf.convert_to_tensor(random_states)\n",
        "\n",
        "\n",
        "step_counter=0\n",
        "metric = {\"episode\": [], \"length\": [], \"total_reward\": [], \"avg_q\": [], \"exploration\": []}\n",
        "for episode in range (episodes):\n",
        "  done=False\n",
        "  state,_=env.reset()\n",
        "  total_rewards=0\n",
        "  episode_length=0\n",
        "  while not done:\n",
        "    # print(f\"State: {state}, Type: {type(state)}\")\n",
        "    action=policy(state,epsilon)\n",
        "    next_state,_,terminated,truncated,info=env.step(action.numpy())\n",
        "    reward= calculate_reward(next_state)\n",
        "    done=terminated or truncated\n",
        "    insert_transition([state,action,reward,next_state,done])\n",
        "    state=next_state\n",
        "    step_counter+=1\n",
        "\n",
        "    if step_counter % LEARN_AFTER_STEPS == 0:\n",
        "      # train the Q-network\n",
        "      current_states,actions,rewards,next_states,terminals=sample_transition(batch_size)\n",
        "\n",
        "      next_action_values= tf.reduce_max(target_net(next_states),axis=1)\n",
        "      targets =tf.where(terminals,rewards, rewards+gamma*next_action_values)\n",
        "      with tf.GradientTape() as tape:\n",
        "        preds = q_net(current_states)\n",
        "        batch_nums = tf.range(0, limit=batch_size)\n",
        "        indices =  tf.stack((batch_nums, actions), axis=1)\n",
        "        current_value = tf.gather_nd(preds, indices)\n",
        "        loss=loss_fn(targets,current_value)\n",
        "      grads=tape.gradient(loss,q_net.trainable_weights)\n",
        "      q_net.optimizer.apply_gradients(zip(grads,q_net.trainable_weights))\n",
        "\n",
        "    if(step_counter%target_update_after==0):\n",
        "      target_net.set_weights(q_net.get_weights())\n",
        "    total_rewards+=reward\n",
        "    episode_length+=1\n",
        "\n",
        "  print(\"Epiosde: \",episode,\" Length: \",episode_length,\" Reward: \",total_rewards, \"Epsilon: \",epsilon)\n",
        "  # Saving Metrics\n",
        "  avg_q = tf.reduce_mean(get_q_values(random_states)).numpy()\n",
        "  metric[\"episode\"].append(episode)\n",
        "  metric[\"length\"].append(episode_length)\n",
        "  metric[\"total_reward\"].append(total_rewards)\n",
        "  metric[\"avg_q\"].append(avg_q)\n",
        "  metric[\"exploration\"].append(epsilon)\n",
        "  epsilon/=epsilon_decay\n",
        "  pd.Dataframe(metric).to_csv(\"metrics.csv\",index=False)\n",
        "env.close()\n",
        "q_net.save(\"dqn_q_net\")"
      ],
      "metadata": {
        "id": "pPyY2osROKyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16826b17-167b-4064-d222-34c2e15c1b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epiosde:  0  Length:  18  Reward:  18.0 Epsilon:  1.0\n",
            "Epiosde:  1  Length:  48  Reward:  48.0 Epsilon:  0.9950248756218907\n",
            "Epiosde:  2  Length:  41  Reward:  41.0 Epsilon:  0.990074503106359\n",
            "Epiosde:  3  Length:  26  Reward:  26.0 Epsilon:  0.98514875930981\n",
            "Epiosde:  4  Length:  78  Reward:  78.0 Epsilon:  0.9802475217013037\n",
            "Epiosde:  5  Length:  19  Reward:  19.0 Epsilon:  0.9753706683595063\n",
            "Epiosde:  6  Length:  10  Reward:  10.0 Epsilon:  0.970518077969658\n",
            "Epiosde:  7  Length:  21  Reward:  21.0 Epsilon:  0.9656896298205554\n",
            "Epiosde:  8  Length:  17  Reward:  17.0 Epsilon:  0.9608852038015477\n",
            "Epiosde:  9  Length:  19  Reward:  19.0 Epsilon:  0.9561046803995501\n",
            "Epiosde:  10  Length:  24  Reward:  24.0 Epsilon:  0.9513479406960699\n",
            "Epiosde:  11  Length:  34  Reward:  34.0 Epsilon:  0.9466148663642487\n",
            "Epiosde:  12  Length:  27  Reward:  27.0 Epsilon:  0.9419053396659193\n",
            "Epiosde:  13  Length:  16  Reward:  16.0 Epsilon:  0.937219243448676\n",
            "Epiosde:  14  Length:  13  Reward:  13.0 Epsilon:  0.9325564611429613\n",
            "Epiosde:  15  Length:  18  Reward:  18.0 Epsilon:  0.9279168767591656\n",
            "Epiosde:  16  Length:  31  Reward:  31.0 Epsilon:  0.9233003748847419\n",
            "Epiosde:  17  Length:  36  Reward:  36.0 Epsilon:  0.9187068406813353\n",
            "Epiosde:  18  Length:  23  Reward:  23.0 Epsilon:  0.9141361598819259\n",
            "Epiosde:  19  Length:  11  Reward:  11.0 Epsilon:  0.909588218787986\n",
            "Epiosde:  20  Length:  65  Reward:  65.0 Epsilon:  0.9050629042666528\n",
            "Epiosde:  21  Length:  15  Reward:  15.0 Epsilon:  0.9005601037479133\n",
            "Epiosde:  22  Length:  15  Reward:  15.0 Epsilon:  0.8960797052218044\n",
            "Epiosde:  23  Length:  50  Reward:  50.0 Epsilon:  0.8916215972356263\n",
            "Epiosde:  24  Length:  13  Reward:  13.0 Epsilon:  0.8871856688911706\n",
            "Epiosde:  25  Length:  20  Reward:  20.0 Epsilon:  0.8827718098419609\n",
            "Epiosde:  26  Length:  12  Reward:  12.0 Epsilon:  0.8783799102905084\n",
            "Epiosde:  27  Length:  30  Reward:  30.0 Epsilon:  0.8740098609855806\n",
            "Epiosde:  28  Length:  62  Reward:  62.0 Epsilon:  0.8696615532194834\n",
            "Epiosde:  29  Length:  28  Reward:  28.0 Epsilon:  0.8653348788253566\n",
            "Epiosde:  30  Length:  27  Reward:  27.0 Epsilon:  0.8610297301744844\n",
            "Epiosde:  31  Length:  77  Reward:  77.0 Epsilon:  0.8567460001736164\n",
            "Epiosde:  32  Length:  27  Reward:  27.0 Epsilon:  0.8524835822623049\n",
            "Epiosde:  33  Length:  18  Reward:  18.0 Epsilon:  0.8482423704102537\n",
            "Epiosde:  34  Length:  26  Reward:  26.0 Epsilon:  0.8440222591146804\n",
            "Epiosde:  35  Length:  14  Reward:  14.0 Epsilon:  0.839823143397692\n",
            "Epiosde:  36  Length:  12  Reward:  12.0 Epsilon:  0.8356449188036738\n",
            "Epiosde:  37  Length:  23  Reward:  23.0 Epsilon:  0.8314874813966904\n",
            "Epiosde:  38  Length:  33  Reward:  33.0 Epsilon:  0.827350727757901\n",
            "Epiosde:  39  Length:  21  Reward:  21.0 Epsilon:  0.8232345549829861\n",
            "Epiosde:  40  Length:  40  Reward:  40.0 Epsilon:  0.8191388606795883\n",
            "Epiosde:  41  Length:  13  Reward:  13.0 Epsilon:  0.8150635429647646\n",
            "Epiosde:  42  Length:  12  Reward:  12.0 Epsilon:  0.8110085004624524\n",
            "Epiosde:  43  Length:  51  Reward:  51.0 Epsilon:  0.8069736323009478\n",
            "Epiosde:  44  Length:  55  Reward:  55.0 Epsilon:  0.8029588381103959\n",
            "Epiosde:  45  Length:  47  Reward:  47.0 Epsilon:  0.7989640180202945\n",
            "Epiosde:  46  Length:  24  Reward:  24.0 Epsilon:  0.7949890726570096\n",
            "Epiosde:  47  Length:  25  Reward:  25.0 Epsilon:  0.7910339031413032\n",
            "Epiosde:  48  Length:  96  Reward:  96.0 Epsilon:  0.7870984110858739\n",
            "Epiosde:  49  Length:  26  Reward:  26.0 Epsilon:  0.7831824985929094\n",
            "Epiosde:  50  Length:  44  Reward:  44.0 Epsilon:  0.7792860682516513\n",
            "Epiosde:  51  Length:  71  Reward:  71.0 Epsilon:  0.7754090231359715\n",
            "Epiosde:  52  Length:  19  Reward:  19.0 Epsilon:  0.7715512668019617\n",
            "Epiosde:  53  Length:  30  Reward:  30.0 Epsilon:  0.7677127032855341\n",
            "Epiosde:  54  Length:  22  Reward:  22.0 Epsilon:  0.763893237100034\n",
            "Epiosde:  55  Length:  21  Reward:  21.0 Epsilon:  0.7600927732338647\n",
            "Epiosde:  56  Length:  52  Reward:  52.0 Epsilon:  0.7563112171481242\n",
            "Epiosde:  57  Length:  54  Reward:  54.0 Epsilon:  0.752548474774253\n",
            "Epiosde:  58  Length:  11  Reward:  11.0 Epsilon:  0.7488044525116946\n",
            "Epiosde:  59  Length:  45  Reward:  45.0 Epsilon:  0.7450790572255668\n",
            "Epiosde:  60  Length:  21  Reward:  21.0 Epsilon:  0.7413721962443451\n",
            "Epiosde:  61  Length:  41  Reward:  41.0 Epsilon:  0.7376837773575574\n",
            "Epiosde:  62  Length:  64  Reward:  64.0 Epsilon:  0.73401370881349\n",
            "Epiosde:  63  Length:  49  Reward:  49.0 Epsilon:  0.7303618993169055\n",
            "Epiosde:  64  Length:  70  Reward:  70.0 Epsilon:  0.7267282580267718\n",
            "Epiosde:  65  Length:  94  Reward:  94.0 Epsilon:  0.7231126945540018\n",
            "Epiosde:  66  Length:  66  Reward:  66.0 Epsilon:  0.719515118959206\n",
            "Epiosde:  67  Length:  30  Reward:  30.0 Epsilon:  0.7159354417504538\n",
            "Epiosde:  68  Length:  97  Reward:  97.0 Epsilon:  0.7123735738810486\n",
            "Epiosde:  69  Length:  78  Reward:  78.0 Epsilon:  0.7088294267473121\n",
            "Epiosde:  70  Length:  125  Reward:  125.0 Epsilon:  0.7053029121863803\n",
            "Epiosde:  71  Length:  45  Reward:  45.0 Epsilon:  0.7017939424740103\n",
            "Epiosde:  72  Length:  12  Reward:  12.0 Epsilon:  0.6983024303223984\n",
            "Epiosde:  73  Length:  36  Reward:  36.0 Epsilon:  0.6948282888780084\n",
            "Epiosde:  74  Length:  71  Reward:  71.0 Epsilon:  0.6913714317194114\n",
            "Epiosde:  75  Length:  42  Reward:  42.0 Epsilon:  0.6879317728551358\n",
            "Epiosde:  76  Length:  135  Reward:  135.0 Epsilon:  0.6845092267215283\n",
            "Epiosde:  77  Length:  38  Reward:  38.0 Epsilon:  0.6811037081806252\n",
            "Epiosde:  78  Length:  42  Reward:  42.0 Epsilon:  0.6777151325180351\n",
            "Epiosde:  79  Length:  44  Reward:  44.0 Epsilon:  0.674343415440831\n",
            "Epiosde:  80  Length:  91  Reward:  91.0 Epsilon:  0.6709884730754538\n",
            "Epiosde:  81  Length:  138  Reward:  138.0 Epsilon:  0.6676502219656257\n",
            "Epiosde:  82  Length:  23  Reward:  23.0 Epsilon:  0.6643285790702744\n",
            "Epiosde:  83  Length:  93  Reward:  93.0 Epsilon:  0.6610234617614672\n",
            "Epiosde:  84  Length:  117  Reward:  117.0 Epsilon:  0.6577347878223555\n",
            "Epiosde:  85  Length:  88  Reward:  88.0 Epsilon:  0.6544624754451299\n",
            "Epiosde:  86  Length:  134  Reward:  134.0 Epsilon:  0.651206443228985\n",
            "Epiosde:  87  Length:  81  Reward:  81.0 Epsilon:  0.6479666101780946\n",
            "Epiosde:  88  Length:  45  Reward:  45.0 Epsilon:  0.6447428956995968\n",
            "Epiosde:  89  Length:  57  Reward:  57.0 Epsilon:  0.6415352196015889\n",
            "Epiosde:  90  Length:  21  Reward:  21.0 Epsilon:  0.6383435020911333\n",
            "Epiosde:  91  Length:  28  Reward:  28.0 Epsilon:  0.6351676637722721\n",
            "Epiosde:  92  Length:  24  Reward:  24.0 Epsilon:  0.6320076256440519\n",
            "Epiosde:  93  Length:  61  Reward:  61.0 Epsilon:  0.6288633090985591\n",
            "Epiosde:  94  Length:  65  Reward:  65.0 Epsilon:  0.6257346359189644\n",
            "Epiosde:  95  Length:  53  Reward:  53.0 Epsilon:  0.6226215282775766\n",
            "Epiosde:  96  Length:  103  Reward:  103.0 Epsilon:  0.6195239087339071\n",
            "Epiosde:  97  Length:  40  Reward:  40.0 Epsilon:  0.6164417002327434\n",
            "Epiosde:  98  Length:  15  Reward:  15.0 Epsilon:  0.6133748261022323\n",
            "Epiosde:  99  Length:  43  Reward:  43.0 Epsilon:  0.6103232100519724\n",
            "Epiosde:  100  Length:  112  Reward:  112.0 Epsilon:  0.6072867761711169\n",
            "Epiosde:  101  Length:  82  Reward:  82.0 Epsilon:  0.6042654489264846\n",
            "Epiosde:  102  Length:  48  Reward:  48.0 Epsilon:  0.6012591531606812\n",
            "Epiosde:  103  Length:  35  Reward:  35.0 Epsilon:  0.5982678140902301\n",
            "Epiosde:  104  Length:  17  Reward:  17.0 Epsilon:  0.5952913573037116\n",
            "Epiosde:  105  Length:  48  Reward:  48.0 Epsilon:  0.5923297087599121\n",
            "Epiosde:  106  Length:  49  Reward:  49.0 Epsilon:  0.5893827947859822\n",
            "Epiosde:  107  Length:  88  Reward:  88.0 Epsilon:  0.5864505420756042\n",
            "Epiosde:  108  Length:  28  Reward:  28.0 Epsilon:  0.5835328776871684\n",
            "Epiosde:  109  Length:  53  Reward:  53.0 Epsilon:  0.5806297290419588\n",
            "Epiosde:  110  Length:  125  Reward:  125.0 Epsilon:  0.577741023922347\n",
            "Epiosde:  111  Length:  71  Reward:  71.0 Epsilon:  0.5748666904699972\n",
            "Epiosde:  112  Length:  57  Reward:  57.0 Epsilon:  0.5720066571840768\n",
            "Epiosde:  113  Length:  70  Reward:  70.0 Epsilon:  0.5691608529194795\n",
            "Epiosde:  114  Length:  140  Reward:  140.0 Epsilon:  0.5663292068850543\n",
            "Epiosde:  115  Length:  145  Reward:  145.0 Epsilon:  0.5635116486418451\n",
            "Epiosde:  116  Length:  158  Reward:  158.0 Epsilon:  0.5607081081013385\n",
            "Epiosde:  117  Length:  17  Reward:  17.0 Epsilon:  0.55791851552372\n",
            "Epiosde:  118  Length:  41  Reward:  41.0 Epsilon:  0.5551428015161394\n",
            "Epiosde:  119  Length:  168  Reward:  168.0 Epsilon:  0.5523808970309845\n",
            "Epiosde:  120  Length:  151  Reward:  151.0 Epsilon:  0.5496327333641637\n",
            "Epiosde:  121  Length:  34  Reward:  34.0 Epsilon:  0.5468982421533968\n",
            "Epiosde:  122  Length:  326  Reward:  326.0 Epsilon:  0.5441773553765142\n",
            "Epiosde:  123  Length:  240  Reward:  240.0 Epsilon:  0.5414700053497654\n",
            "Epiosde:  124  Length:  53  Reward:  53.0 Epsilon:  0.5387761247261348\n",
            "Epiosde:  125  Length:  108  Reward:  108.0 Epsilon:  0.5360956464936665\n",
            "Epiosde:  126  Length:  108  Reward:  108.0 Epsilon:  0.5334285039737976\n",
            "Epiosde:  127  Length:  16  Reward:  16.0 Epsilon:  0.5307746308196991\n",
            "Epiosde:  128  Length:  102  Reward:  102.0 Epsilon:  0.5281339610146261\n",
            "Epiosde:  129  Length:  187  Reward:  187.0 Epsilon:  0.5255064288702748\n",
            "Epiosde:  130  Length:  20  Reward:  20.0 Epsilon:  0.5228919690251491\n",
            "Epiosde:  131  Length:  133  Reward:  133.0 Epsilon:  0.5202905164429344\n",
            "Epiosde:  132  Length:  164  Reward:  164.0 Epsilon:  0.5177020064108802\n",
            "Epiosde:  133  Length:  242  Reward:  242.0 Epsilon:  0.5151263745381892\n",
            "Epiosde:  134  Length:  148  Reward:  148.0 Epsilon:  0.5125635567544172\n",
            "Epiosde:  135  Length:  153  Reward:  153.0 Epsilon:  0.5100134893078779\n",
            "Epiosde:  136  Length:  15  Reward:  15.0 Epsilon:  0.5074761087640577\n",
            "Epiosde:  137  Length:  93  Reward:  93.0 Epsilon:  0.5049513520040375\n",
            "Epiosde:  138  Length:  123  Reward:  123.0 Epsilon:  0.5024391562229229\n",
            "Epiosde:  139  Length:  11  Reward:  11.0 Epsilon:  0.49993945892828157\n",
            "Epiosde:  140  Length:  182  Reward:  182.0 Epsilon:  0.4974521979385887\n",
            "Epiosde:  141  Length:  172  Reward:  172.0 Epsilon:  0.49497731138168033\n",
            "Epiosde:  142  Length:  132  Reward:  132.0 Epsilon:  0.4925147376932143\n",
            "Epiosde:  143  Length:  240  Reward:  240.0 Epsilon:  0.49006441561513864\n",
            "Epiosde:  144  Length:  84  Reward:  84.0 Epsilon:  0.48762628419416787\n",
            "Epiosde:  145  Length:  233  Reward:  233.0 Epsilon:  0.4852002827802666\n",
            "Epiosde:  146  Length:  151  Reward:  151.0 Epsilon:  0.48278635102514095\n",
            "Epiosde:  147  Length:  205  Reward:  205.0 Epsilon:  0.4803844288807373\n",
            "Epiosde:  148  Length:  265  Reward:  265.0 Epsilon:  0.4779944565977486\n",
            "Epiosde:  149  Length:  214  Reward:  214.0 Epsilon:  0.475616374724128\n",
            "Epiosde:  150  Length:  210  Reward:  210.0 Epsilon:  0.47325012410361\n",
            "Epiosde:  151  Length:  296  Reward:  296.0 Epsilon:  0.4708956458742389\n",
            "Epiosde:  152  Length:  170  Reward:  170.0 Epsilon:  0.4685528814669044\n",
            "Epiosde:  153  Length:  97  Reward:  97.0 Epsilon:  0.46622177260388503\n",
            "Epiosde:  154  Length:  38  Reward:  38.0 Epsilon:  0.4639022612973981\n",
            "Epiosde:  155  Length:  179  Reward:  179.0 Epsilon:  0.46159428984815737\n",
            "Epiosde:  156  Length:  111  Reward:  111.0 Epsilon:  0.45929780084393773\n",
            "Epiosde:  157  Length:  90  Reward:  90.0 Epsilon:  0.45701273715814705\n",
            "Epiosde:  158  Length:  182  Reward:  182.0 Epsilon:  0.4547390419484051\n",
            "Epiosde:  159  Length:  133  Reward:  133.0 Epsilon:  0.4524766586551295\n",
            "Epiosde:  160  Length:  208  Reward:  208.0 Epsilon:  0.4502255310001289\n",
            "Epiosde:  161  Length:  281  Reward:  281.0 Epsilon:  0.44798560298520296\n",
            "Epiosde:  162  Length:  252  Reward:  252.0 Epsilon:  0.44575681889074925\n",
            "Epiosde:  163  Length:  68  Reward:  68.0 Epsilon:  0.4435391232743774\n",
            "Epiosde:  164  Length:  180  Reward:  180.0 Epsilon:  0.44133246096952977\n",
            "Epiosde:  165  Length:  195  Reward:  195.0 Epsilon:  0.43913677708410925\n",
            "Epiosde:  166  Length:  61  Reward:  61.0 Epsilon:  0.43695201699911373\n",
            "Epiosde:  167  Length:  192  Reward:  192.0 Epsilon:  0.4347781263672774\n",
            "Epiosde:  168  Length:  110  Reward:  110.0 Epsilon:  0.4326150511117189\n",
            "Epiosde:  169  Length:  96  Reward:  96.0 Epsilon:  0.43046273742459595\n",
            "Epiosde:  170  Length:  172  Reward:  172.0 Epsilon:  0.42832113176576714\n",
            "Epiosde:  171  Length:  32  Reward:  32.0 Epsilon:  0.4261901808614599\n",
            "Epiosde:  172  Length:  245  Reward:  245.0 Epsilon:  0.4240698317029452\n",
            "Epiosde:  173  Length:  214  Reward:  214.0 Epsilon:  0.42196003154521916\n",
            "Epiosde:  174  Length:  283  Reward:  283.0 Epsilon:  0.41986072790569073\n",
            "Epiosde:  175  Length:  241  Reward:  241.0 Epsilon:  0.4177718685628764\n",
            "Epiosde:  176  Length:  132  Reward:  132.0 Epsilon:  0.4156934015551009\n",
            "Epiosde:  177  Length:  116  Reward:  116.0 Epsilon:  0.41362527517920494\n",
            "Epiosde:  178  Length:  232  Reward:  232.0 Epsilon:  0.4115674379892587\n",
            "Epiosde:  179  Length:  83  Reward:  83.0 Epsilon:  0.40951983879528236\n",
            "Epiosde:  180  Length:  211  Reward:  211.0 Epsilon:  0.40748242666197254\n",
            "Epiosde:  181  Length:  115  Reward:  115.0 Epsilon:  0.4054551509074354\n",
            "Epiosde:  182  Length:  169  Reward:  169.0 Epsilon:  0.4034379611019258\n",
            "Epiosde:  183  Length:  170  Reward:  170.0 Epsilon:  0.4014308070665929\n",
            "Epiosde:  184  Length:  195  Reward:  195.0 Epsilon:  0.39943363887223177\n",
            "Epiosde:  185  Length:  74  Reward:  74.0 Epsilon:  0.3974464068380416\n",
            "Epiosde:  186  Length:  166  Reward:  166.0 Epsilon:  0.3954690615303897\n",
            "Epiosde:  187  Length:  205  Reward:  205.0 Epsilon:  0.3935015537615818\n",
            "Epiosde:  188  Length:  172  Reward:  172.0 Epsilon:  0.3915438345886387\n",
            "Epiosde:  189  Length:  242  Reward:  242.0 Epsilon:  0.3895958553120783\n",
            "Epiosde:  190  Length:  56  Reward:  56.0 Epsilon:  0.3876575674747048\n",
            "Epiosde:  191  Length:  190  Reward:  190.0 Epsilon:  0.38572892286040283\n",
            "Epiosde:  192  Length:  167  Reward:  167.0 Epsilon:  0.38380987349293816\n",
            "Epiosde:  193  Length:  224  Reward:  224.0 Epsilon:  0.38190037163476437\n",
            "Epiosde:  194  Length:  126  Reward:  126.0 Epsilon:  0.38000036978583523\n",
            "Epiosde:  195  Length:  148  Reward:  148.0 Epsilon:  0.37810982068242316\n",
            "Epiosde:  196  Length:  222  Reward:  222.0 Epsilon:  0.3762286772959435\n",
            "Epiosde:  197  Length:  169  Reward:  169.0 Epsilon:  0.37435689283178464\n",
            "Epiosde:  198  Length:  214  Reward:  214.0 Epsilon:  0.372494420728144\n",
            "Epiosde:  199  Length:  69  Reward:  69.0 Epsilon:  0.37064121465486966\n",
            "Epiosde:  200  Length:  31  Reward:  31.0 Epsilon:  0.3687972285123082\n",
            "Epiosde:  201  Length:  123  Reward:  123.0 Epsilon:  0.3669624164301574\n",
            "Epiosde:  202  Length:  248  Reward:  248.0 Epsilon:  0.36513673276632586\n",
            "Epiosde:  203  Length:  193  Reward:  193.0 Epsilon:  0.3633201321057969\n",
            "Epiosde:  204  Length:  150  Reward:  150.0 Epsilon:  0.36151256925949943\n",
            "Epiosde:  205  Length:  191  Reward:  191.0 Epsilon:  0.35971399926318354\n",
            "Epiosde:  206  Length:  223  Reward:  223.0 Epsilon:  0.35792437737630206\n",
            "Epiosde:  207  Length:  172  Reward:  172.0 Epsilon:  0.3561436590808976\n",
            "Epiosde:  208  Length:  173  Reward:  173.0 Epsilon:  0.3543718000804952\n",
            "Epiosde:  209  Length:  159  Reward:  159.0 Epsilon:  0.35260875629900024\n",
            "Epiosde:  210  Length:  44  Reward:  44.0 Epsilon:  0.3508544838796023\n",
            "Epiosde:  211  Length:  153  Reward:  153.0 Epsilon:  0.3491089391836839\n",
            "Epiosde:  212  Length:  176  Reward:  176.0 Epsilon:  0.34737207878973525\n",
            "Epiosde:  213  Length:  65  Reward:  65.0 Epsilon:  0.34564385949227394\n",
            "Epiosde:  214  Length:  134  Reward:  134.0 Epsilon:  0.3439242383007701\n",
            "Epiosde:  215  Length:  159  Reward:  159.0 Epsilon:  0.3422131724385773\n",
            "Epiosde:  216  Length:  12  Reward:  12.0 Epsilon:  0.340510619341868\n",
            "Epiosde:  217  Length:  167  Reward:  167.0 Epsilon:  0.33881653665857514\n",
            "Epiosde:  218  Length:  14  Reward:  14.0 Epsilon:  0.3371308822473385\n",
            "Epiosde:  219  Length:  152  Reward:  152.0 Epsilon:  0.33545361417645625\n",
            "Epiosde:  220  Length:  55  Reward:  55.0 Epsilon:  0.33378469072284206\n",
            "Epiosde:  221  Length:  84  Reward:  84.0 Epsilon:  0.3321240703709872\n",
            "Epiosde:  222  Length:  14  Reward:  14.0 Epsilon:  0.3304717118119276\n",
            "Epiosde:  223  Length:  23  Reward:  23.0 Epsilon:  0.3288275739422165\n",
            "Epiosde:  224  Length:  38  Reward:  38.0 Epsilon:  0.327191615862902\n",
            "Epiosde:  225  Length:  200  Reward:  200.0 Epsilon:  0.3255637968785095\n",
            "Epiosde:  226  Length:  70  Reward:  70.0 Epsilon:  0.3239440764960294\n",
            "Epiosde:  227  Length:  41  Reward:  41.0 Epsilon:  0.3223324144239099\n",
            "Epiosde:  228  Length:  184  Reward:  184.0 Epsilon:  0.3207287705710547\n",
            "Epiosde:  229  Length:  172  Reward:  172.0 Epsilon:  0.3191331050458256\n",
            "Epiosde:  230  Length:  143  Reward:  143.0 Epsilon:  0.31754537815505035\n",
            "Epiosde:  231  Length:  83  Reward:  83.0 Epsilon:  0.3159655504030352\n",
            "Epiosde:  232  Length:  188  Reward:  188.0 Epsilon:  0.3143935824905823\n",
            "Epiosde:  233  Length:  162  Reward:  162.0 Epsilon:  0.31282943531401225\n",
            "Epiosde:  234  Length:  166  Reward:  166.0 Epsilon:  0.31127306996419135\n",
            "Epiosde:  235  Length:  162  Reward:  162.0 Epsilon:  0.30972444772556357\n",
            "Epiosde:  236  Length:  156  Reward:  156.0 Epsilon:  0.3081835300751877\n",
            "Epiosde:  237  Length:  175  Reward:  175.0 Epsilon:  0.3066502786817788\n",
            "Epiosde:  238  Length:  170  Reward:  170.0 Epsilon:  0.30512465540475503\n",
            "Epiosde:  239  Length:  154  Reward:  154.0 Epsilon:  0.3036066222932886\n",
            "Epiosde:  240  Length:  122  Reward:  122.0 Epsilon:  0.30209614158536185\n",
            "Epiosde:  241  Length:  153  Reward:  153.0 Epsilon:  0.30059317570682775\n",
            "Epiosde:  242  Length:  192  Reward:  192.0 Epsilon:  0.2990976872704754\n",
            "Epiosde:  243  Length:  187  Reward:  187.0 Epsilon:  0.29760963907509996\n",
            "Epiosde:  244  Length:  29  Reward:  29.0 Epsilon:  0.2961289941045771\n",
            "Epiosde:  245  Length:  136  Reward:  136.0 Epsilon:  0.2946557155269424\n",
            "Epiosde:  246  Length:  67  Reward:  67.0 Epsilon:  0.2931897666934751\n",
            "Epiosde:  247  Length:  162  Reward:  162.0 Epsilon:  0.29173111113778616\n",
            "Epiosde:  248  Length:  35  Reward:  35.0 Epsilon:  0.29027971257491164\n",
            "Epiosde:  249  Length:  60  Reward:  60.0 Epsilon:  0.28883553490040964\n",
            "Epiosde:  250  Length:  126  Reward:  126.0 Epsilon:  0.28739854218946237\n",
            "Epiosde:  251  Length:  24  Reward:  24.0 Epsilon:  0.2859686986959825\n",
            "Epiosde:  252  Length:  58  Reward:  58.0 Epsilon:  0.28454596885172395\n",
            "Epiosde:  253  Length:  169  Reward:  169.0 Epsilon:  0.283130317265397\n",
            "Epiosde:  254  Length:  151  Reward:  151.0 Epsilon:  0.2817217087217881\n",
            "Epiosde:  255  Length:  28  Reward:  28.0 Epsilon:  0.2803201081808837\n",
            "Epiosde:  256  Length:  69  Reward:  69.0 Epsilon:  0.27892548077699875\n",
            "Epiosde:  257  Length:  28  Reward:  28.0 Epsilon:  0.2775377918179092\n",
            "Epiosde:  258  Length:  52  Reward:  52.0 Epsilon:  0.2761570067839893\n",
            "Epiosde:  259  Length:  115  Reward:  115.0 Epsilon:  0.27478309132735257\n",
            "Epiosde:  260  Length:  136  Reward:  136.0 Epsilon:  0.2734160112709976\n",
            "Epiosde:  261  Length:  179  Reward:  179.0 Epsilon:  0.27205573260795785\n",
            "Epiosde:  262  Length:  91  Reward:  91.0 Epsilon:  0.2707022215004556\n",
            "Epiosde:  263  Length:  100  Reward:  100.0 Epsilon:  0.26935544427906033\n",
            "Epiosde:  264  Length:  129  Reward:  129.0 Epsilon:  0.2680153674418511\n",
            "Epiosde:  265  Length:  158  Reward:  158.0 Epsilon:  0.2666819576535832\n",
            "Epiosde:  266  Length:  107  Reward:  107.0 Epsilon:  0.26535518174485895\n",
            "Epiosde:  267  Length:  48  Reward:  48.0 Epsilon:  0.2640350067113025\n",
            "Epiosde:  268  Length:  132  Reward:  132.0 Epsilon:  0.26272139971273883\n",
            "Epiosde:  269  Length:  81  Reward:  81.0 Epsilon:  0.26141432807237697\n",
            "Epiosde:  270  Length:  130  Reward:  130.0 Epsilon:  0.260113759275997\n",
            "Epiosde:  271  Length:  77  Reward:  77.0 Epsilon:  0.25881966097114134\n",
            "Epiosde:  272  Length:  65  Reward:  65.0 Epsilon:  0.2575320009663098\n",
            "Epiosde:  273  Length:  239  Reward:  239.0 Epsilon:  0.256250747230159\n",
            "Epiosde:  274  Length:  109  Reward:  109.0 Epsilon:  0.2549758678907055\n",
            "Epiosde:  275  Length:  239  Reward:  239.0 Epsilon:  0.2537073312345328\n",
            "Epiosde:  276  Length:  68  Reward:  68.0 Epsilon:  0.2524451057060028\n",
            "Epiosde:  277  Length:  108  Reward:  108.0 Epsilon:  0.2511891599064705\n",
            "Epiosde:  278  Length:  166  Reward:  166.0 Epsilon:  0.24993946259350303\n",
            "Epiosde:  279  Length:  217  Reward:  217.0 Epsilon:  0.24869598268010254\n",
            "Epiosde:  280  Length:  55  Reward:  55.0 Epsilon:  0.2474586892339329\n",
            "Epiosde:  281  Length:  151  Reward:  151.0 Epsilon:  0.24622755147655018\n",
            "Epiosde:  282  Length:  230  Reward:  230.0 Epsilon:  0.24500253878263703\n",
            "Epiosde:  283  Length:  500  Reward:  500.0 Epsilon:  0.24378362067924086\n",
            "Epiosde:  284  Length:  189  Reward:  189.0 Epsilon:  0.2425707668450158\n",
            "Epiosde:  285  Length:  83  Reward:  83.0 Epsilon:  0.24136394710946849\n",
            "Epiosde:  286  Length:  199  Reward:  199.0 Epsilon:  0.24016313145220747\n",
            "Epiosde:  287  Length:  500  Reward:  500.0 Epsilon:  0.23896829000219652\n",
            "Epiosde:  288  Length:  181  Reward:  181.0 Epsilon:  0.23777939303701148\n",
            "Epiosde:  289  Length:  59  Reward:  59.0 Epsilon:  0.236596410982101\n",
            "Epiosde:  290  Length:  373  Reward:  373.0 Epsilon:  0.23541931441005076\n",
            "Epiosde:  291  Length:  410  Reward:  410.0 Epsilon:  0.23424807403985154\n",
            "Epiosde:  292  Length:  500  Reward:  500.0 Epsilon:  0.2330826607361707\n",
            "Epiosde:  293  Length:  500  Reward:  500.0 Epsilon:  0.23192304550862758\n",
            "Epiosde:  294  Length:  500  Reward:  500.0 Epsilon:  0.23076919951107225\n",
            "Epiosde:  295  Length:  229  Reward:  229.0 Epsilon:  0.22962109404086795\n",
            "Epiosde:  296  Length:  169  Reward:  169.0 Epsilon:  0.2284787005381771\n",
            "Epiosde:  297  Length:  103  Reward:  103.0 Epsilon:  0.22734199058525087\n",
            "Epiosde:  298  Length:  141  Reward:  141.0 Epsilon:  0.22621093590572228\n",
            "Epiosde:  299  Length:  341  Reward:  341.0 Epsilon:  0.2250855083639028\n",
            "Epiosde:  300  Length:  386  Reward:  386.0 Epsilon:  0.2239656799640824\n",
            "Epiosde:  301  Length:  500  Reward:  500.0 Epsilon:  0.22285142284983325\n",
            "Epiosde:  302  Length:  500  Reward:  500.0 Epsilon:  0.2217427093033167\n",
            "Epiosde:  303  Length:  500  Reward:  500.0 Epsilon:  0.22063951174459373\n",
            "Epiosde:  304  Length:  67  Reward:  67.0 Epsilon:  0.21954180273093907\n",
            "Epiosde:  305  Length:  319  Reward:  319.0 Epsilon:  0.2184495549561583\n",
            "Epiosde:  306  Length:  500  Reward:  500.0 Epsilon:  0.2173627412499088\n",
            "Epiosde:  307  Length:  55  Reward:  55.0 Epsilon:  0.2162813345770237\n",
            "Epiosde:  308  Length:  500  Reward:  500.0 Epsilon:  0.21520530803683952\n",
            "Epiosde:  309  Length:  95  Reward:  95.0 Epsilon:  0.2141346348625269\n",
            "Epiosde:  310  Length:  500  Reward:  500.0 Epsilon:  0.2130692884204248\n",
            "Epiosde:  311  Length:  299  Reward:  299.0 Epsilon:  0.21200924220937795\n",
            "Epiosde:  312  Length:  194  Reward:  194.0 Epsilon:  0.21095446986007757\n",
            "Epiosde:  313  Length:  497  Reward:  497.0 Epsilon:  0.20990494513440555\n",
            "Epiosde:  314  Length:  86  Reward:  86.0 Epsilon:  0.20886064192478165\n",
            "Epiosde:  315  Length:  171  Reward:  171.0 Epsilon:  0.2078215342535141\n",
            "Epiosde:  316  Length:  226  Reward:  226.0 Epsilon:  0.20678759627215335\n",
            "Epiosde:  317  Length:  319  Reward:  319.0 Epsilon:  0.2057588022608491\n",
            "Epiosde:  318  Length:  218  Reward:  218.0 Epsilon:  0.20473512662771057\n",
            "Epiosde:  319  Length:  260  Reward:  260.0 Epsilon:  0.20371654390816973\n",
            "Epiosde:  320  Length:  424  Reward:  424.0 Epsilon:  0.202703028764348\n",
            "Epiosde:  321  Length:  34  Reward:  34.0 Epsilon:  0.2016945559844259\n",
            "Epiosde:  322  Length:  11  Reward:  11.0 Epsilon:  0.20069110048201586\n",
            "Epiosde:  323  Length:  145  Reward:  145.0 Epsilon:  0.19969263729553818\n",
            "Epiosde:  324  Length:  131  Reward:  131.0 Epsilon:  0.19869914158760021\n",
            "Epiosde:  325  Length:  15  Reward:  15.0 Epsilon:  0.19771058864437835\n",
            "Epiosde:  326  Length:  16  Reward:  16.0 Epsilon:  0.19672695387500336\n",
            "Epiosde:  327  Length:  16  Reward:  16.0 Epsilon:  0.19574821281094865\n",
            "Epiosde:  328  Length:  154  Reward:  154.0 Epsilon:  0.19477434110542155\n",
            "Epiosde:  329  Length:  105  Reward:  105.0 Epsilon:  0.1938053145327578\n",
            "Epiosde:  330  Length:  324  Reward:  324.0 Epsilon:  0.19284110898781873\n",
            "Epiosde:  331  Length:  184  Reward:  184.0 Epsilon:  0.1918817004853918\n",
            "Epiosde:  332  Length:  58  Reward:  58.0 Epsilon:  0.19092706515959385\n",
            "Epiosde:  333  Length:  88  Reward:  88.0 Epsilon:  0.18997717926327748\n",
            "Epiosde:  334  Length:  500  Reward:  500.0 Epsilon:  0.1890320191674403\n",
            "Epiosde:  335  Length:  50  Reward:  50.0 Epsilon:  0.18809156136063715\n",
            "Epiosde:  336  Length:  126  Reward:  126.0 Epsilon:  0.1871557824483952\n",
            "Epiosde:  337  Length:  208  Reward:  208.0 Epsilon:  0.18622465915263206\n",
            "Epiosde:  338  Length:  33  Reward:  33.0 Epsilon:  0.1852981683110767\n",
            "Epiosde:  339  Length:  74  Reward:  74.0 Epsilon:  0.18437628687669325\n",
            "Epiosde:  340  Length:  220  Reward:  220.0 Epsilon:  0.18345899191710774\n",
            "Epiosde:  341  Length:  58  Reward:  58.0 Epsilon:  0.18254626061403756\n",
            "Epiosde:  342  Length:  316  Reward:  316.0 Epsilon:  0.18163807026272397\n",
            "Epiosde:  343  Length:  227  Reward:  227.0 Epsilon:  0.18073439827136714\n",
            "Epiosde:  344  Length:  239  Reward:  239.0 Epsilon:  0.17983522216056433\n",
            "Epiosde:  345  Length:  134  Reward:  134.0 Epsilon:  0.1789405195627506\n",
            "Epiosde:  346  Length:  369  Reward:  369.0 Epsilon:  0.17805026822164238\n",
            "Epiosde:  347  Length:  260  Reward:  260.0 Epsilon:  0.177164445991684\n",
            "Epiosde:  348  Length:  385  Reward:  385.0 Epsilon:  0.17628303083749652\n",
            "Epiosde:  349  Length:  292  Reward:  292.0 Epsilon:  0.17540600083332988\n",
            "Epiosde:  350  Length:  323  Reward:  323.0 Epsilon:  0.17453333416251732\n",
            "Epiosde:  351  Length:  288  Reward:  288.0 Epsilon:  0.17366500911693267\n",
            "Epiosde:  352  Length:  215  Reward:  215.0 Epsilon:  0.17280100409645044\n",
            "Epiosde:  353  Length:  276  Reward:  276.0 Epsilon:  0.17194129760840843\n",
            "Epiosde:  354  Length:  95  Reward:  95.0 Epsilon:  0.1710858682670731\n",
            "Epiosde:  355  Length:  172  Reward:  172.0 Epsilon:  0.17023469479310757\n",
            "Epiosde:  356  Length:  313  Reward:  313.0 Epsilon:  0.16938775601304237\n",
            "Epiosde:  357  Length:  267  Reward:  267.0 Epsilon:  0.16854503085874864\n",
            "Epiosde:  358  Length:  403  Reward:  403.0 Epsilon:  0.1677064983669141\n",
            "Epiosde:  359  Length:  305  Reward:  305.0 Epsilon:  0.1668721376785215\n",
            "Epiosde:  360  Length:  272  Reward:  272.0 Epsilon:  0.16604192803832987\n",
            "Epiosde:  361  Length:  500  Reward:  500.0 Epsilon:  0.1652158487943581\n",
            "Epiosde:  362  Length:  500  Reward:  500.0 Epsilon:  0.16439387939737127\n",
            "Epiosde:  363  Length:  500  Reward:  500.0 Epsilon:  0.16357599940036943\n",
            "Epiosde:  364  Length:  500  Reward:  500.0 Epsilon:  0.16276218845807905\n",
            "Epiosde:  365  Length:  457  Reward:  457.0 Epsilon:  0.16195242632644682\n",
            "Epiosde:  366  Length:  500  Reward:  500.0 Epsilon:  0.16114669286213615\n",
            "Epiosde:  367  Length:  400  Reward:  400.0 Epsilon:  0.16034496802202602\n",
            "Epiosde:  368  Length:  100  Reward:  100.0 Epsilon:  0.15954723186271247\n",
            "Epiosde:  369  Length:  433  Reward:  433.0 Epsilon:  0.15875346454001243\n",
            "Epiosde:  370  Length:  180  Reward:  180.0 Epsilon:  0.1579636463084701\n",
            "Epiosde:  371  Length:  108  Reward:  108.0 Epsilon:  0.1571777575208658\n",
            "Epiosde:  372  Length:  288  Reward:  288.0 Epsilon:  0.1563957786277272\n",
            "Epiosde:  373  Length:  14  Reward:  14.0 Epsilon:  0.155617690176843\n",
            "Epiosde:  374  Length:  189  Reward:  189.0 Epsilon:  0.1548434728127791\n",
            "Epiosde:  375  Length:  500  Reward:  500.0 Epsilon:  0.15407310727639714\n",
            "Epiosde:  376  Length:  234  Reward:  234.0 Epsilon:  0.15330657440437528\n",
            "Epiosde:  377  Length:  500  Reward:  500.0 Epsilon:  0.15254385512873164\n",
            "Epiosde:  378  Length:  255  Reward:  255.0 Epsilon:  0.15178493047634992\n",
            "Epiosde:  379  Length:  378  Reward:  378.0 Epsilon:  0.15102978156850738\n",
            "Epiosde:  380  Length:  500  Reward:  500.0 Epsilon:  0.15027838962040538\n",
            "Epiosde:  381  Length:  270  Reward:  270.0 Epsilon:  0.14953073594070188\n",
            "Epiosde:  382  Length:  224  Reward:  224.0 Epsilon:  0.14878680193104665\n",
            "Epiosde:  383  Length:  198  Reward:  198.0 Epsilon:  0.14804656908561858\n",
            "Epiosde:  384  Length:  500  Reward:  500.0 Epsilon:  0.1473100189906653\n",
            "Epiosde:  385  Length:  11  Reward:  11.0 Epsilon:  0.14657713332404507\n",
            "Epiosde:  386  Length:  455  Reward:  455.0 Epsilon:  0.14584789385477123\n",
            "Epiosde:  387  Length:  500  Reward:  500.0 Epsilon:  0.14512228244255845\n",
            "Epiosde:  388  Length:  500  Reward:  500.0 Epsilon:  0.1444002810373716\n",
            "Epiosde:  389  Length:  500  Reward:  500.0 Epsilon:  0.14368187167897672\n",
            "Epiosde:  390  Length:  500  Reward:  500.0 Epsilon:  0.14296703649649428\n",
            "Epiosde:  391  Length:  500  Reward:  500.0 Epsilon:  0.1422557577079545\n",
            "Epiosde:  392  Length:  500  Reward:  500.0 Epsilon:  0.14154801761985525\n",
            "Epiosde:  393  Length:  500  Reward:  500.0 Epsilon:  0.14084379862672167\n",
            "Epiosde:  394  Length:  500  Reward:  500.0 Epsilon:  0.14014308321066835\n",
            "Epiosde:  395  Length:  62  Reward:  62.0 Epsilon:  0.13944585394096354\n",
            "Epiosde:  396  Length:  112  Reward:  112.0 Epsilon:  0.13875209347359557\n",
            "Epiosde:  397  Length:  500  Reward:  500.0 Epsilon:  0.1380617845508414\n",
            "Epiosde:  398  Length:  500  Reward:  500.0 Epsilon:  0.1373749100008372\n",
            "Epiosde:  399  Length:  500  Reward:  500.0 Epsilon:  0.13669145273715147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why Do We Need This Training Step?**\n",
        "The goal of reinforcement learning is to make the Q-network better at predicting the \"value\" of taking certain actions in certain states.\n",
        "\n",
        "- We want the Q-network's predicted Q-values (`preds`) to match the \"target\" Q-values (`targets`), which represent what the true value should ideally be.\n",
        "- This training process is like telling the Q-network: *\"Hey, you guessed this, but the correct answer should be that. Update yourself to be more accurate next time!\"*\n",
        "\n",
        "---\n",
        "\n",
        "### **What Are We Doing Here?**\n",
        "\n",
        "1. **Predict the Value of the Next State (Future Rewards):**\n",
        "   ```python\n",
        "   next_action_values = tf.reduce_max(target_net(next_states), axis=1)\n",
        "   ```\n",
        "   - Using the **target network**, we look at the next states (`next_states`) from our sampled transitions and predict the Q-values for all possible actions in those states.\n",
        "   - `tf.reduce_max` takes the maximum Q-value for each next state. This is because we assume the agent will act optimally in the future (choose the best action).\n",
        "\n",
        "2. **Calculate Target Q-Values for the Current Step:**\n",
        "   ```python\n",
        "   targets = tf.where(terminals, rewards, rewards + gamma * next_action_values)\n",
        "   ```\n",
        "   - If the sampled transition ends in a terminal state (`terminals` is `True`), the target Q-value is simply the `reward` since there are no future rewards to consider.\n",
        "   - If the transition is not terminal, the target Q-value is:\n",
        "     \\[\n",
        "     \\text{Target} = \\text{Reward} + \\gamma \\times \\text{(Max Q-value of the next state)}\n",
        "     \\]\n",
        "     - `gamma` is the discount factor, which ensures that rewards further in the future are less important than immediate rewards.\n",
        "\n",
        "3. **Predict the Q-Values for Current States:**\n",
        "   ```python\n",
        "   preds = q_net(current_states)\n",
        "   ```\n",
        "   - The **main network** (`q_net`) predicts Q-values for all actions in the `current_states`.\n",
        "\n",
        "4. **Extract the Q-Values for the Actions Taken:**\n",
        "   ```python\n",
        "   batch_nums = tf.range(0, limit=batch_size)\n",
        "   indices = tf.stack((batch_nums, actions), axis=1)\n",
        "   current_value = tf.gather_nd(preds, indices)\n",
        "   ```\n",
        "   - The network predicted Q-values for **all possible actions**, but we only care about the Q-values for the actions that were actually taken in our sampled transitions.\n",
        "   - Here’s how we extract those specific Q-values:\n",
        "     - `batch_nums` generates indices for the batch of transitions (e.g., `[0, 1, 2, ...]`).\n",
        "     - `actions` are the actions that were taken in the transitions.\n",
        "     - `tf.stack((batch_nums, actions), axis=1)` creates indices to identify the specific Q-values corresponding to the actions in each transition.\n",
        "     - `tf.gather_nd(preds, indices)` retrieves the Q-values for those actions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Are We Doing This?**\n",
        "\n",
        "We now have two things:\n",
        "1. **Targets**: What the Q-values *should* be (based on the Bellman equation and the target network).\n",
        "2. **Predictions**: What the Q-network *currently* thinks the Q-values are.\n",
        "\n",
        "The next step (not shown in your code snippet) would typically involve calculating the **loss** between these two values and using it to update the Q-network. For example:\n",
        "```python\n",
        "loss = tf.reduce_mean((targets - current_value) ** 2)\n",
        "optimizer.minimize(loss, q_net.trainable_variables)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **In Layman Terms**\n",
        "\n",
        "After collecting experience from the environment, we:\n",
        "1. Use the **target network** to estimate the \"correct answers\" (targets) for the Q-values.\n",
        "2. Use the **main network** to get the current guesses (predictions) for the Q-values.\n",
        "3. Compare the two and update the main network so its guesses become more accurate."
      ],
      "metadata": {
        "id": "LykjWt1wOd2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluator"
      ],
      "metadata": {
        "id": "Juro3UFTMbVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(5):\n",
        "  done = False\n",
        "  observation,_=env.reset()\n",
        "  state= tf.convert_to_tensor([observation])\n",
        "  while not done:\n",
        "    frame=env.render()\n",
        "    clear_output(wait=True)\n",
        "    plt.imshow(frame)\n",
        "    plt.axis('off')\n",
        "    display(plt.gcf())\n",
        "    action=policy(state);\n",
        "    state,reward,terminated,truncated,info=env.step(action.numpy())\n",
        "    done=terminated or truncated\n",
        "    state=tf.convert_to_tensor([state])\n",
        "env.close()"
      ],
      "metadata": {
        "id": "tg2wyCJCMfGc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}